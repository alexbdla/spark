{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxbYMTFUiIHq6p+H/59yLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexbdla/spark/blob/main/spark_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Configurações:\n"
      ],
      "metadata": {
        "id": "0AJF3tERqyY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa parte de configurações deverá ser carregada sempre antes de executarmos os demais códigos."
      ],
      "metadata": {
        "id": "DZXlfzWHnsce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mH6O7ELyqILN"
      },
      "outputs": [],
      "source": [
        "# Instalando o Java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando Apache Spark com Hadoop:\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "7r8yY482qMZG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descompactando o arquivo baixado na célula anterior:"
      ],
      "metadata": {
        "id": "DTRUa2j4xkDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "CRL3ad5dwju2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalando FindSpark e PySpark:\n",
        "\n",
        "- __PySpark__ -> interface do Python para o Apache Spark (API para trabalhar com Python)\n",
        "- __FindSpark__ -> adiciona o PySpark ao sys.path (caminho do sistema) em tempo de execução"
      ],
      "metadata": {
        "id": "HCv8zLsTrWVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "smAPAQkMrPUj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "fAnuBuJisbLc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Configurando as variáveis de ambiente:"
      ],
      "metadata": {
        "id": "K8Prj56pskvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "ra7HQlSMs-62"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FindSpark\n",
        "FindSpark é uma biblioteca Python que permite aos usuários conectar seu programa Python ao Apache Spark. Ele é projetado para ajudar a configurar as variáveis de ambiente necessárias e inicializar o SparkContext. O FindSpark pode ser instalado usando o pip, um gerenciador de pacotes Python. Ele é particularmente útil quando se trabalha com o Spark em um ambiente de desenvolvimento local ou em um notebook Jupyter. Com o FindSpark, os usuários podem escrever aplicativos Spark em Python, incluindo PySpark e SparkSQL. Além disso, o FindSpark também ajuda a configurar a integração com outras bibliotecas populares de ciência de dados em Python, como Pandas, Numpy e Matplotlib."
      ],
      "metadata": {
        "id": "mxDBKnHaoi2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "# Iniciando o pacote:\n",
        "findspark.init();"
      ],
      "metadata": {
        "id": "yF22S7iWs_9K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Começando a trabalhar com o Spark"
      ],
      "metadata": {
        "id": "NqxKE4mktinW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importação de dados"
      ],
      "metadata": {
        "id": "-xrc31uYyPIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext() \n",
        "print(spark_contexto)           \n",
        "print(spark_contexto.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K07gdHGoyXV",
        "outputId": "101f05d7-2139-4c94-9596-98b83f842715"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "3.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Criar/obter uma seção do Spark na máquina local, também cria o contexto\n",
        "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate(); \n",
        "\n",
        "spark = SparkSession.builder.getOrCreate();\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygytHxL6yvzg",
        "outputId": "a050ae66-bc0b-472d-abb4-9516a45331a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7fcc00ffac10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv('/content/sample_data/california_housing_test.csv', inferSchema=True, header=True);"
      ],
      "metadata": {
        "id": "OWscbHbxKACY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.printSchema();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv56J0guLLoh",
        "outputId": "224a9feb-b24b-4009-b038-5f711222980a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeira linha desse conjunto de dados:\n",
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiCh9cA-M1JV",
        "outputId": "348b777a-ae27-4131-baa2-02a6a2d22c04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(longitude=-122.05, latitude=37.37, housing_median_age=27.0, total_rooms=3885.0, total_bedrooms=661.0, population=1537.0, households=606.0, median_income=6.6085, median_house_value=344700.0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantidade de linhas:\n",
        "dataset.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjWGZI-ldTTT",
        "outputId": "fd7364d5-29b0-4c60-9f82-8b02706aef7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando uma tabela temporária com os dados do Dataset:\n",
        "dataset.createOrReplaceTempView(\"tabela_temporaria\")"
      ],
      "metadata": {
        "id": "jH5e-gERdTHl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimindo as tabelas no catálogo:\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7SHyp6KdSyQ",
        "outputId": "3484935f-298a-4b04-840b-7c4f99bc489b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consulta SQL (3 registros com os dados referentes às colunas de latitude e longitude):\n",
        "query = \"FROM tabela_temporaria SELECT longitude, latitude LIMIT 3\"\n",
        "saida = spark.sql(query)\n",
        "saida.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGKbZ1zXa2hn",
        "outputId": "066f6fe2-6541-455c-a3c8-5f525a0fa692"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|longitude|latitude|\n",
            "+---------+--------+\n",
            "|  -122.05|   37.37|\n",
            "|   -118.3|   34.26|\n",
            "|  -117.81|   33.78|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convertendo Spark SQL para Pandas:"
      ],
      "metadata": {
        "id": "Xv8uVWSKf8HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz uma consulta SQL na tabela temporária para encontrar o valor máximo da coluna \"total_rooms\" na \"tabela_temporaria\" e renomeia esse valor máximo para \"maximo_quartos\":\n",
        "query1 = \"SELECT MAX(total_rooms) as maximo_quartos FROM tabela_temporaria\""
      ],
      "metadata": {
        "id": "Uz3y5peggDrf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executa a consulta SQL no Spark, obtendo um DataFrame do Spark:\n",
        "q_maximo_quartos = spark.sql(query1)\n",
        "# Converte o resultado para um DataFrame do Pandas:\n",
        "pd_maximo_quartos = q_maximo_quartos.toPandas()\n",
        "# Converte o valor do DataFrame para um inteiro:\n",
        "qtd_maximo_quartos = int(pd_maximo_quartos.loc[0, 'maximo_quartos'])\n",
        "# Imprime o resultado da consulta:\n",
        "print('A quantidade máxima de quartos é: {}'.format(qtd_maximo_quartos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3wzhZBjiYMc",
        "outputId": "eb0b503b-bb61-4ffe-8438-c17cf8c776ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A quantidade máxima de quartos é: 30450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Localizando o bloco residencial com o maior nº de quartos:\n",
        "query2 = \"SELECT longitude, latitude FROM tabela_temporaria WHERE total_rooms = \" + str(qtd_maximo_quartos)\n",
        "localizacao_maximo_quartos = spark.sql(query2)\n",
        "pd_localizacao_maximo_quartos = localizacao_maximo_quartos.toPandas()\n",
        "print(pd_localizacao_maximo_quartos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPemjLZiiX5A",
        "outputId": "bed8398e-4c74-4bd7-e505-dd2309bb9abb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   longitude  latitude\n",
            "0     -117.2     33.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processo de conversão:"
      ],
      "metadata": {
        "id": "YFkY0pa3q3De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "media = 0 \n",
        "desvio_padrao = 0.1\n",
        "# Cria um DataFrame do Pandas com 100 valores, no formato de uma distribuição normal com média 0 e desvio padrão 0.1:\n",
        "pd_temporario = pd.DataFrame(np.random.normal(media, desvio_padrao, 100))\n",
        "# Transforma o DataFrame do Pandas em um DataFrame do Spark:\n",
        "spark_temporario = spark.createDataFrame(pd_temporario)\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xCSc1Iansyj",
        "outputId": "6b256ab9-857b-4623-9a2a-6a8dc7a46b6a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_temporario.createOrReplaceTempView(\"nova_tabela_temporaria\")\n",
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "qt_MIGnlnstV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7377055a-e8cd-488c-f345-dc07d69cb1bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Table(name='nova_tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='tabela_temporaria', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fechando a seção do Spark: \n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "ulrbE6EWNc5s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - MapReduce:"
      ],
      "metadata": {
        "id": "sjTjvUCWkSEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MapReduce é um modelo de programação usado para processamento distribuído de grandes conjuntos de dados em um cluster de computadores. Ele foi criado pelo Google e é amplamente usado no processamento de grandes conjuntos de dados em vários ambientes, incluindo Hadoop, Spark e outros sistemas de processamento de Big Data.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "O MapReduce funciona em duas fases principais: a fase de mapeamento (map) e a fase de redução (reduce). Na fase de mapeamento, os dados são divididos em partes menores e independentes, que são processadas em paralelo por diferentes nós do cluster. Em seguida, os resultados são combinados e ordenados por chave. Na fase de redução, os dados são processados novamente e os resultados são consolidados.\n",
        "\n",
        "O MapReduce é eficiente para lidar com grandes conjuntos de dados, pois o processamento pode ser distribuído em vários nós do cluster, o que permite que os dados sejam processados ​​em paralelo. Ele é usado em várias aplicações de Big Data, incluindo análise de dados, mineração de dados, aprendizado de máquina e processamento de linguagem natural, entre outras."
      ],
      "metadata": {
        "id": "syPVvo-Io5t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1:"
      ],
      "metadata": {
        "id": "b7BhsA9MkmYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "uGEUl7XXnsor"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "pJO-RqFunsiD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "o5LYljVwnsXm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize(vetor)"
      ],
      "metadata": {
        "id": "ukuTWSE5lLb6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paralelo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS0HuLGNruqk",
        "outputId": "d0ba1040-7a3d-48b0-e9b1-6e987656ffb2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x ** 2 + x)"
      ],
      "metadata": {
        "id": "c7k_B1sYlWos"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sg4jnjAlgcN",
        "outputId": "8602baf2-6137-4ad9-f2d7-50144c047cdc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 420, 930, 1640, 2550]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2:"
      ],
      "metadata": {
        "id": "BULYh1PIpce5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize([\"distribuida\", \"distribuida\", \"spark\", \"rdd\", \"spark\", \"spark\"])"
      ],
      "metadata": {
        "id": "9FtiN9A5pmEk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "funcao_lambda = lambda x : (x, 1)"
      ],
      "metadata": {
        "id": "iPoiTzzJpkgX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "mapa = paralelo.map(funcao_lambda).reduceByKey(add).collect()"
      ],
      "metadata": {
        "id": "atn8101lpkW1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for(w, c) in mapa:\n",
        "  print(\"{}: {}\".format(w, c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1jnw93WpkO6",
        "outputId": "155f71bf-d4ed-420c-de6b-450524483b50"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribuida: 2\n",
            "spark: 3\n",
            "rdd: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "FsoCAcZKpkBi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3:"
      ],
      "metadata": {
        "id": "wO1PAGwksMoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark_contexto = SparkContext()"
      ],
      "metadata": {
        "id": "rUtpjtAtsQ8H"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vetor = np.array([10, 20, 30, 40, 50])"
      ],
      "metadata": {
        "id": "w_b3IPXzsQy9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paralelo = spark_contexto.parallelize(vetor)"
      ],
      "metadata": {
        "id": "9Gr2utNvsQmw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add"
      ],
      "metadata": {
        "id": "DKGiCM7b4k1P"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapa = paralelo.map(lambda x : x ** 4 - 10 * x ** 2 + 3).reduce(lambda a, b: a + b)\n",
        "mapa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQfvQ-Um3rJw",
        "outputId": "1343ec36-b658-4d20-e888-8de72788548d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9735015"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_contexto.stop()"
      ],
      "metadata": {
        "id": "Ef670yha6wGB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 4:"
      ],
      "metadata": {
        "id": "6GoR69pC63MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext('local', 'MapReduce')"
      ],
      "metadata": {
        "id": "gMkBGJL568fX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1, 2, 3, 4, 5, 3]\n",
        "lista_rdd = sc.parallelize(lista)"
      ],
      "metadata": {
        "id": "bX_N9yC969Xu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOy-g9hE--_q",
        "outputId": "580bc417-f045-48fd-e7aa-c3d88a068189"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "par_ordenado = lambda numero : (numero, numero * 10)"
      ],
      "metadata": {
        "id": "IVXQu9K__DRT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`map` -> função que permite aplicar uma transformação em cada elemento do array resultando em um novo array."
      ],
      "metadata": {
        "id": "GXImpA01CnLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.map(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNQwlvZoCRoU",
        "outputId": "b63f4ac9-5458-47a7-be0b-a03ccd1d2f1e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 10), (2, 20), (3, 30), (4, 40), (5, 50), (3, 30)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`flatMap` ->  função flatMap realiza um map de uma função sobre uma coleção de dados, porém achatando o resultado final em um nível, isto é, retornando um array de uma dimensão apenas."
      ],
      "metadata": {
        "id": "hUYlCZ6hBqBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lista_rdd.flatMap(par_ordenado).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya0jZXCx_NxT",
        "outputId": "9e77d058-0972-4f66-ad8b-3ef91f24c2a3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 3, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "q74HdMca_dmP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 5:"
      ],
      "metadata": {
        "id": "5wkdDyTVDWbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext('local', 'MapReduce')"
      ],
      "metadata": {
        "id": "NttmYjh7FWew"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = [1, 2, 3, 4, 5, 3]\n",
        "rdd = sc.parallelize(lista)"
      ],
      "metadata": {
        "id": "iwFtuo5TFtOE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.map(lambda x : (x, x ** 2)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4klinUEF9PM",
        "outputId": "0f067674-36f5-49ef-9926-2cd72e374c86"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25), (3, 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "ayi_mdeqGoZ5"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}